{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLhViib5BZY5"
   },
   "source": [
    " - Deep Learning \n",
    "\n",
    "- Code for all the tasks must be written in this notebook (you do not need to submit any other files).\n",
    "- The output of all cells must be present in the version of the notebook you submit.\n",
    "- The university honor code should be maintained. Any violation, if found, will result in disciplinary action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "4yumnMGXB3BW",
    "outputId": "c7ec076c-f88d-47c5-e335-d5af53bace0b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04U3nfBMChDT"
   },
   "outputs": [],
   "source": [
    "!ls \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykgJRObJwLvt"
   },
   "outputs": [],
   "source": [
    "# !unzip -e \"/content/drive/My Drive/UCMerced_LandUse.zip\" -d \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wF8ywfmdBZY9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import seaborn as sns\n",
    "from keras.models import load_model\n",
    "from keras.applications import vgg16\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import seaborn\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import Flatten, Dense, Dropout, Input,Conv1D,Reshape\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F77QjZieBZZH",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this assignment you will be exploring a few important concepts used in the deep learning projects:\n",
    "- Working with satellite imagery data\n",
    "- Dataset annotation\n",
    "- Fine-tuning / Transfer Learning\n",
    "- Unsupervised feature representation with Autoencoder\n",
    "- Comparison of end-to-end trained model with finetuned model\n",
    "\n",
    "We will be using two datasets, the links are provided to you. You will also be working with three pretrained models, which have been provided to you. You are **highly** encouraged to explore the datasets and model architectures in order to get the most out of this assignment. \n",
    "\n",
    "**_Datasets:_**\n",
    "- Brick Kiln (Nepal) - available [here](https://drive.google.com/drive/folders/1dQEA0fxepVnELPnz-gAAYFb9hJSV4Azc)\n",
    "- UC Merced Land Use - available [here](http://weegee.vision.ucmerced.edu/datasets/landuse.html)\n",
    "\n",
    "**_Pretrained Models:_** \n",
    "Can be found [here](https://drive.google.com/open?id=1Ekvk3JUW3eI5sgITxzNklXrJXnCfTKBu)\n",
    "- ResNet18 pretrained on Brick Kiln (Lahore) - available as `InceptionResNet-v2-2classes`\n",
    "- Autoencoder pretrained on GT Cross View and fine tuned on UC Merced - available as `encoder_gt`\n",
    "- VGG16 pretrained on ImageNet - available in `keras.applications` (consult relevant documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "naHoDdArBZZI"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Let's start with a binary classification problem. \n",
    "\n",
    "The Brick Kiln (Nepal) dataset you have been given consists of 100 tiles at zoom level 17. A script to break up these tiles into 64 sub-tiles of zoom 20 has also been given to you. Your job is to:\n",
    "- Split 100 images into 6400 images using the script\n",
    "- Manually annotate the dataset by moving the kiln pictures into one folder and non-kiln picutures into other folder.\n",
    "- Code up a generator to properly load the images and corresponding binary labels into a model. You have to resize images into 224X224X3\n",
    "\n",
    "*Scale images between 0 and 1 and apply mean subtraction in the generator*\n",
    "\n",
    "*Each of you has been given unique 100 tiles, so for the love of God do not get annotated data from someone else.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yifQ1kD7BZZK"
   },
   "outputs": [],
   "source": [
    "def preprocessing_meanShift (images):\n",
    "    #Add code here\n",
    "    processed_images=images\n",
    "    imgs=np.stack(images)\n",
    "    mean = np.mean(images[:,:,:,0]),np.mean(images[:,:,:,1]),np.mean(images[:,:,:,2])\n",
    "    std = np.std(images[:,:,:,0]),np.std(images[:,:,:,1]),np.std(images[:,:,:,2])\n",
    "#     print (\"mean is : \",mean)\n",
    "    processed_images=(processed_images-mean)/std\n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "7dSuQxJQBZZN",
    "outputId": "b377ea6d-f39d-423d-be05-3ce7c3ad8d05"
   },
   "outputs": [],
   "source": [
    "kiln_path = 'kilns/'\n",
    "non_kiln_path = 'non_kilns/'\n",
    "batch_size=50\n",
    "input_shape=(256,256,3)\n",
    "\n",
    "kiln_paths =  glob.glob(kiln_path +'*'+'.jpg')\n",
    "kiln_labels = [1 for i in range(len(kiln_paths))]     #Assigned 1 to kiln images\n",
    "non_kiln_paths =  glob.glob(non_kiln_path +'*'+'.jpg')\n",
    "non_kiln_labels = [0 for i in range(len(non_kiln_paths))]   # Assigned 0 to the non_kiln images\n",
    "print(non_kiln_paths)\n",
    "num_classes=2\n",
    "all_paths = non_kiln_paths + kiln_paths\n",
    "all_labels =  non_kiln_labels + kiln_labels \n",
    "# print(all_labels)\n",
    "\n",
    "y_true = np.zeros((len(all_labels), 2))\n",
    "# print(len(all_labels))\n",
    "\n",
    "\n",
    "def brick_kiln_generator(all_paths,all_labels, y_true,bath_size=128):\n",
    "    batch_start = 0\n",
    "    num_classes=2\n",
    "    batch_end = batch_size\n",
    "    n=len(all_labels)\n",
    "    indexes=np.arange(0,n,batch_size)\n",
    "    np.random.shuffle(indexes)\n",
    "#     print(\"indexes: \",indexes)\n",
    "    \n",
    "    if n % batch_size != 0:\n",
    "        indexes = indexes[:-1] \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        index=0\n",
    "        for b_start in indexes:\n",
    "                \n",
    "            batch_image_paths = all_paths[b_start: b_start+batch_size]\n",
    "            batch_labels =  all_labels[b_start: b_start+batch_size]\n",
    "            \n",
    "            batch_images=np.zeros((batch_size, *input_shape))\n",
    "            batch_labels_enc = np.zeros((batch_size , num_classes))\n",
    "            \n",
    "            count=0\n",
    "            for path in batch_image_paths:\n",
    "                \n",
    "                img=cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)\n",
    "#                 img = cv2.resize(img,(224,224))        #Image resized as 224x224x3\n",
    "        \n",
    "#                 print(\"image is: \",img.shape)\n",
    "\n",
    "                img.astype(float)\n",
    "                batch_images[count,:,:,:]=img\n",
    "                \n",
    "                labels= np.zeros((num_classes,),dtype='int')\n",
    "                labels[batch_labels[count]]=1\n",
    "                batch_labels_enc[count,:]=labels\n",
    "#                 print(index)\n",
    "                y_true[index,:]=labels      # This appends the true labels\n",
    "                count+=1\n",
    "                index+=1\n",
    "                \n",
    "            batch_images = preprocessing_meanShift(batch_images)\n",
    "#             y_true += batch_labels_enc\n",
    "            yield(batch_images,batch_labels_enc)\n",
    "            print(\"batch has been yielded\")\n",
    "\n",
    "# brick_kiln_generator(all_paths,all_labels,batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex7ObwIbBZZS"
   },
   "source": [
    "## Task 2\n",
    "\n",
    "Now you will evaluate performance of a pretrained (on Brick Kiln Lahore dataset) ResNet18 model using the generator made in Task1. You will:\n",
    "- Obtain predictions for the entire dataset\n",
    "- Construct a binary confusion matrix and visualize it as a heatmap\n",
    "\n",
    "*You can use scikit-learn's `metrics.confusion_matrix` function. Consult the relevant documentation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 28512
    },
    "colab_type": "code",
    "id": "e8_6AkexBZZT",
    "outputId": "76357b5b-b184-4317-fbd7-1d29e906c93f"
   },
   "outputs": [],
   "source": [
    "# get confusion matrix usign built-in function \n",
    "model = load_model(\"/content/drive/My Drive/InceptionResNet-v2-2classes.h5\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2448
    },
    "colab_type": "code",
    "id": "5nl47Wq6BZZW",
    "outputId": "751e4dff-37aa-48c8-8135-2800cd4bf7ad"
   },
   "outputs": [],
   "source": [
    "testGen = brick_kiln_generator(all_paths,all_labels,y_true,batch_size)\n",
    "y_pred=model.predict_generator(testGen, steps=len(all_paths)//batch_size,verbose=1)\n",
    "# result=model.predict(testGen, batch_size, verbose=1, steps=len(all_paths)//batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JfnDa0qxg1e2",
    "outputId": "b6cbc4f6-cc7d-4a6b-c536-db04d20cd9d0"
   },
   "outputs": [],
   "source": [
    "print(len(y_pred),len(y_true))\n",
    "print(len(all_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ewUm9XdFBZZZ"
   },
   "outputs": [],
   "source": [
    "def decodeArray(y_true,y_pred):    #Decode y_true and y_pred into a 1d array\n",
    "    dy_true=[]    #Decoding into a id array\n",
    "    dy_pred=[]  #Decoding into a id array\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i][0]==True):\n",
    "            dy_pred.append(0)\n",
    "        else:\n",
    "            dy_pred.append(1)\n",
    "        \n",
    "    for i in range(len(y_true)):\n",
    "        if(y_true[i][0]==True):\n",
    "            dy_true.append(0)\n",
    "        else:\n",
    "            dy_true.append(1)\n",
    "    \n",
    "    return dy_true,dy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqKqYl7vBZZd"
   },
   "outputs": [],
   "source": [
    "y_pred = (y_pred > 0.5) \n",
    "        \n",
    "dy_true,dy_pred = decodeArray(y_true,y_pred)\n",
    "\n",
    "# print(\"result achieved is: \", y_true[6500:6600])\n",
    "cm=confusion_matrix(dy_true[0:6586],dy_pred[0:6586])\n",
    "seaborn.heatmap(cm,annot=True)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qi4zNDFLBZZf"
   },
   "source": [
    "## Task 3\n",
    "\n",
    "Next you will employ Transfer Learning and finetune the pretrained ResNet18 model you used in Task2 to better fit the Brick Kiln (Nepal) dataset. You will:\n",
    "- Freeze everything except the FC layers and train it using the generator from Task1 (using appropriate hyperparameters)\n",
    "- Construct a binary confusion matrix and visualize it as a heatmap\n",
    "- Compare this confusion matrix with the one made in Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRofQ7lboOks"
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current < self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=0.002, verbose=1),\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONxOaT3rBZZh"
   },
   "outputs": [],
   "source": [
    "#fine tuning. Dense layers are task related(classification). Freeze conv layer. Lr and parameters shall be less\n",
    "#Train till convergence\n",
    "resnet18_pretrained = model\n",
    "# resnet18_pretrained.layers.pop()\n",
    "# task3=resnet18_pretrained(weights=None, include_top=False,input_shape=input_shape, classes=2)\n",
    "# resnet18_pretrained.summary()\n",
    "for layer in resnet18_pretrained.layers[0:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqfpPRXeBZZj"
   },
   "outputs": [],
   "source": [
    "im = Input(shape=input_shape)\n",
    "l = resnet18_pretrained(im)\n",
    "# print(l.shape)\n",
    "# l = Flatten()(l)\n",
    "# l = Dense(1024, activation='relu')(l)\n",
    "# l = Dropout(0.5)(l)\n",
    "# output = Dense(num_classes, activation='softmax')(l)\n",
    "\n",
    "custom_model = Model(im, l)\n",
    "# resnet18_pretrained.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Ba3mCNSFBZZm",
    "outputId": "c6daceff-71a4-4f8d-b407-985a9f1d9d77"
   },
   "outputs": [],
   "source": [
    "custom_model.summary()\n",
    "adam=Adam(lr=0.001)\n",
    "custom_model.compile(optimizer=adam,\n",
    "              loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "TPZhqySxBZZo",
    "outputId": "c944ce39-2871-4764-fbea-59558f3c9e09"
   },
   "outputs": [],
   "source": [
    "epochs=2\n",
    "train_gen=brick_kiln_generator(all_paths,all_labels,y_true,batch_size)\n",
    "hist1 = custom_model.fit_generator(train_gen, epochs=epochs, steps_per_epoch=len(all_paths)//batch_size,verbose=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ty6jLDwXBZZs"
   },
   "outputs": [],
   "source": [
    "testGen = brick_kiln_generator(all_paths,all_labels,y_true,batch_size)\n",
    "y_pred_cust=custom_model.predict_generator(testGen, steps=len(all_paths)//batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mePx2EekBZZu"
   },
   "outputs": [],
   "source": [
    "y_pred_cust = (y_pred_cust > 0.5) \n",
    "        \n",
    "dy_true,dy_pred = decodeArray(y_true,y_pred_cust)\n",
    "\n",
    "# print(\"result achieved is: \",result.shape, y_true.shape)\n",
    "cm=confusion_matrix(dy_true,dy_pred)\n",
    "seaborn.heatmap(cm,annot=True)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "McJhr1viBZZw"
   },
   "source": [
    "## Task 4\n",
    "\n",
    "Now we will look at a multiclass classification problem.\n",
    "\n",
    "The UC Merced Land Use dataset consists of 21 classes, ranging from airplanes to forests to tennis courts. Let's add kilns to it since you worked so hard to annotate the dataset in Task1. You will:\n",
    "- Download the dataset and add a new folder (following the already existing folder structure) corresponding to brick kilns\n",
    "- Code up a generator to properly load the images and corresponding 22-class labels into a model. You have to resize images into 224X224X3 for VGG16\n",
    "\n",
    "*Scale images between 0 and 1 and apply mean subtraction in the generator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EF9Avn0BZZx"
   },
   "outputs": [],
   "source": [
    "# classesDir = \"drive/My Drive/UCMerced_LandUse/Images\"\n",
    "# cdir = os.getcwd()\n",
    "# os.chdir(cdir+\"/\"+classesDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "s0zdGvVaBZZz",
    "outputId": "ff66c343-3d54-499a-b3aa-1ded337f02ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  []\n"
     ]
    }
   ],
   "source": [
    "# print(cdir)\n",
    "classesDir = \"drive/My Drive/UCMerced_LandUse/Images/\"\n",
    "# os.chdir(cdir+classesDir)\n",
    "classes= glob.glob(classesDir+ '*')\n",
    "print(\"classes: \",classes)\n",
    "# os.chdir(cdir)\n",
    "num_classes=len(classes)\n",
    "inputShape = (224,224,3)\n",
    "batch_size=79\n",
    "\n",
    "allPaths = []\n",
    "allLabels = []\n",
    "count=0\n",
    "for cdir in classes:\n",
    "    path = glob.glob(cdir+\"/\"+'*')\n",
    "    allPaths+=path\n",
    "    labels= [count for i in range(len(path))]\n",
    "    allLabels+=labels\n",
    "    count+=1;\n",
    "# print(allPaths)\n",
    "# print(allLabels)\n",
    "y_true = np.zeros((len(allLabels), num_classes))\n",
    "\n",
    "def land_use_generator(allPaths,allLabels, y_true,num_classes,inpShape,resize=True,bath_size=128):\n",
    "    batch_start = 0\n",
    "    batch_end = batch_size\n",
    "    n=len(allLabels)\n",
    "    indexes=np.arange(0,n,batch_size)\n",
    "    np.random.shuffle(indexes)\n",
    "#     print(\"indexes: \",indexes)\n",
    "    \n",
    "    if n % batch_size != 0:\n",
    "        indexes = indexes[:-1] \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        index=0\n",
    "        for b_start in indexes:\n",
    "                \n",
    "            batch_image_paths = allPaths[b_start: b_start+batch_size]\n",
    "            batch_labels =  allLabels[b_start: b_start+batch_size]\n",
    "            \n",
    "            batch_images=np.zeros((batch_size, *inpShape))\n",
    "            batch_labels_enc = np.zeros((batch_size , num_classes))\n",
    "            \n",
    "            count=0\n",
    "            for path in batch_image_paths:\n",
    "#                 print(path)\n",
    "                img=cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "                if(resize==True):                          # Will only do resize for part 4 not 6\n",
    "                    img = cv2.resize(img,(inpShape[0],inpShape[1]))        #Image resized as 224x224x3\n",
    "        \n",
    "#                 print(\"image is: \",img.shape)\n",
    "\n",
    "#                 img.astype(float)\n",
    "#                 print(img.shape)\n",
    "                batch_images[count,:,:,:]=img\n",
    "                \n",
    "                labels= np.zeros((num_classes,),dtype='int')\n",
    "                labels[batch_labels[count]]=1\n",
    "                batch_labels_enc[count,:]=labels\n",
    "#                 print(index)\n",
    "                y_true[index,:]=labels      # This appends the true labels\n",
    "                count+=1\n",
    "                index+=1\n",
    "                \n",
    "            batch_images = preprocessing_meanShift(batch_images)\n",
    "#             y_true += batch_labels_enc\n",
    "            yield(batch_images,batch_labels_enc)\n",
    "            print(\"batch has been yielded\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viPb9hjCBZZ3"
   },
   "source": [
    "## Task 5\n",
    "\n",
    "Next you will again employ Transfer Learning and finetune the pretrained (on ImageNet) VGG16 to better fit the modified Land Use dataset. You will:\n",
    "- Change the number of nodes in the last FC layer according to the number of classes i.e. 22 \n",
    "- Freeze everything except the FC layers and train it using the generator from Task4 (using appropriate hyperparameters)\n",
    "- Construct a multi-class confusion matrix and visualize it as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "dhoy7hGWBZZ4",
    "outputId": "3ffdaedb-b297-4b4f-e3db-63dd5564bcc7"
   },
   "outputs": [],
   "source": [
    "#Now different task plus data set(unlike perv). Remove last dense layer and add new layer. Activation softmax and out \n",
    "#nuber of classes. \n",
    "\n",
    "\n",
    "vgg_imagenet = vgg16.VGG16(include_top=False, weights='imagenet')\n",
    "for l in vgg_imagenet.layers:\n",
    "#     l.freeze = True\n",
    "    l.trainable = False\n",
    "# vgg_imagenet.summary()\n",
    "# add new FC layers here\n",
    "\n",
    "\n",
    "im = Input(shape=inputShape)\n",
    "l = vgg_imagenet(im)\n",
    "l = Flatten()(l)\n",
    "l = Dense(512, activation='relu')(l)\n",
    "l = Dropout(0.2)(l)\n",
    "output = Dense(num_classes, activation='softmax')(l)\n",
    "\n",
    "vggModel = Model(im, output)\n",
    "print(\"model created\")\n",
    "# print summary and compile\n",
    "vggModel.summary()\n",
    "vggModel.compile(optimizer='adam',loss='mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1357
    },
    "colab_type": "code",
    "id": "lJHw0DM9BZZ_",
    "outputId": "823d3bcf-45df-47cf-e1e5-8f24012b7e3f"
   },
   "outputs": [],
   "source": [
    "epochs=2\n",
    "train_gen2=land_use_generator(allPaths,allLabels,y_true,num_classes,(224,224,3),True,batch_size)\n",
    "print(len(allPaths))\n",
    "hist2 = vggModel.fit_generator(train_gen2, epochs=epochs, steps_per_epoch=len(allPaths)//batch_size,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "PARABnPm3k8H",
    "outputId": "cc81fcf1-6d17-44b1-d185-1433d27fac0e"
   },
   "outputs": [],
   "source": [
    "pred_gen=land_use_generator(allPaths,allLabels,y_true,num_classes,(224,224,3),True,batch_size)\n",
    "y_pred=vggModel.predict_generator(pred_gen, steps=len(allPaths)//batch_size,verbose=1)\n",
    "\n",
    "# y_pred_cust = (y_pred_cust > 0.5) \n",
    "        \n",
    "# dy_true,dy_pred = decodeArray(y_true,y_pred_cust)\n",
    "\n",
    "# # print(\"result achieved is: \",result.shape, y_true.shape)\n",
    "# cm=confusion_matrix(dy_true,dy_pred)\n",
    "# seaborn.heatmap(cm,annot=True)\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "oWco0YB56z57",
    "outputId": "19229e28-59f3-4d06-f5e1-d0cbcc744939"
   },
   "outputs": [],
   "source": [
    "# print(y_pred[1900:1910])\n",
    "\n",
    "x= [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0. ,0.],\n",
    "   [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0. ,0.]]\n",
    "\n",
    "# x=array(x)\n",
    "\n",
    "# x=x.flatten()\n",
    "# print(x.shape)\n",
    "\n",
    "# cm=confusion_matrix(x,[0,1],[1,2,3,4,5,6,7,8,9,10])\n",
    "# seaborn.heatmap(cm,annot=True)\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZcFP-MRBZaC"
   },
   "source": [
    "## Task 6\n",
    "\n",
    "Now you will make use of Unsupervised Representation Learning as studied in class. You have been provided with a pretrained autoencoder (just the encoder part) and you will use it to obtain deep features for the modified UC Merced Land Use dataset. You will have to:\n",
    "- Obtain predictions for the entire dataset\n",
    "- Save then in an appropriate fashion\n",
    "\n",
    "*Keep in mind that this model takes input of shape 256X256X3 so you need to resize the images before feeding them into this model*\n",
    "\n",
    "*Try to think about how you could use the generator from Task4 to create another generator which would yield encoded features along with labels instead of raw images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3046
    },
    "colab_type": "code",
    "id": "hKMrTuKTBZaC",
    "outputId": "9c87fb88-52f1-4ec5-e967-13009b580222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 20) 2960        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 20) 80          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 20) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 11584       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 1344        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 128, 64) 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 64) 36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 128, 128, 64) 0           add_1[0][0]                      \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 128, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  73856       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 128)  8320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 128)  0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 128)  147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 128)  147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 128)  0           add_3[0][0]                      \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  295168      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 256)  33024       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  590080      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 256)  0           conv2d_14[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 256)  590080      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 256)  590080      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 256)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 128)  295040      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 128)  32896       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 128)  147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 128)  0           conv2d_19[0][0]                  \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 128)  147584      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 128)  147584      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 128)  0           add_7[0][0]                      \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 128)  512         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 20)     23060       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 20)     80          conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 20)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 20)     2580        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 20)     3620        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 20)     0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 20)     80          add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 20)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 20)     3620        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 20)     80          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 20)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 20)     3620        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 8, 8, 20)     0           add_9[0][0]                      \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 20)     80          add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 20)     0           batch_normalization_21[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,566,836\n",
      "Trainable params: 3,562,028\n",
      "Non-trainable params: 4,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Apple/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "#Encoder != auto-encoder. \n",
    "#Prediction is the deep features.\n",
    "\n",
    "encoder_model = load_model(\"encoder_gt.h5\")\n",
    "encoder_model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IcRBiNAFW6a"
   },
   "outputs": [],
   "source": [
    "encoder_model.compile(optimizer=\"adam\",loss='mse')       # This will create predictions on all land\n",
    "pred_gen=land_use_generator(allPaths,allLabels,y_true,num_classes,(256,256,3),True,batch_size)\n",
    "enc_output=encoder_model.predict_generator(pred_gen, steps=len(allPaths)//batch_size,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "7M6iVea4b8qY",
    "outputId": "91d0086a-567e-4cd9-af17-399e99b9424a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  []\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Same generator but only difference is it also produce \n",
    "classesDir = \"UCMerced_LandUse/Images/\"       \n",
    "# os.chdir(cdir+classesDir)\n",
    "classes= glob.glob(classesDir+ '*')\n",
    "print(\"classes: \",classes)\n",
    "# os.chdir(cdir)\n",
    "num_classes=len(classes)\n",
    "print(num_classes)\n",
    "inputShape = (224,224,3)\n",
    "batch_size=79\n",
    "\n",
    "allPaths = []\n",
    "allLabels = []\n",
    "count=0\n",
    "for cdir in classes:\n",
    "    path = glob.glob(cdir+\"/\"+'*')\n",
    "    allPaths+=path\n",
    "    labels= [count for i in range(len(path))]\n",
    "    allLabels+=labels\n",
    "    count+=1;\n",
    "# print(allPaths)\n",
    "# print(allLabels)\n",
    "y_true = np.zeros((len(allLabels), num_classes))\n",
    "\n",
    "def land_use_generator2(allPaths,allLabels, y_true,num_classes,inpShape,encoder_model,resize=True,bath_size=128):\n",
    "    batch_start = 0\n",
    "    batch_end = batch_size\n",
    "    n=len(allLabels)\n",
    "    indexes=np.arange(0,n,batch_size)\n",
    "    np.random.shuffle(indexes)\n",
    "#     print(\"indexes: \",indexes)\n",
    "    \n",
    "    if n % batch_size != 0:\n",
    "        indexes = indexes[:-1] \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        index=0\n",
    "        for b_start in indexes:\n",
    "                \n",
    "            batch_image_paths = allPaths[b_start: b_start+batch_size]\n",
    "            batch_labels =  allLabels[b_start: b_start+batch_size]\n",
    "            \n",
    "            batch_images=np.zeros((batch_size, *inpShape))\n",
    "            batch_labels_enc = np.zeros((batch_size , num_classes))\n",
    "            \n",
    "            count=0\n",
    "            for path in batch_image_paths:\n",
    "#                 print(path)\n",
    "                img=cv2.cvtColor(cv2.imread(path),cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "                if(resize==True):                          # Will only do resize for part 4 not 6\n",
    "                    img = cv2.resize(img,(inpShape[0],inpShape[1]))        #Image resized as 224x224x3\n",
    "        \n",
    "#                 print(\"image is: \",img.shape)\n",
    "\n",
    "                img.astype(float)\n",
    "#                 print(img.shape)\n",
    "                batch_images[count,:,:,:]=img\n",
    "                \n",
    "                labels= np.zeros((num_classes,),dtype='int')\n",
    "                labels[batch_labels[count]]=1\n",
    "                batch_labels_enc[count,:]=labels\n",
    "#                 print(index)\n",
    "                y_true[index,:]=labels      # This appends the true labels\n",
    "                count+=1\n",
    "                index+=1\n",
    "                \n",
    "            batch_images = preprocessing_meanShift(batch_images)\n",
    "            print(batch_images.shape)\n",
    "            encoded_images=encoder_model.predict_on_batch(batch_images)\n",
    "            print(encoded_images.shape)\n",
    "#             y_true += batch_labels_enc\n",
    "            yield(encoded_images,batch_labels_enc)\n",
    "            print(\"batch has been yielded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkowWSg1BZaF"
   },
   "source": [
    "## Task 7\n",
    "\n",
    "Now you will train a classifier from scratch to discriminate the 22 classes based on the deep features you extracted in Task6. You will:\n",
    "- Train a classifier with the following architecture\n",
    "> 1D conv 3x1 -> 1D conv 3x1 -> FC 256 -> FC 22\n",
    "- Construct a multiclass confusion matrix and visualize it as a heatmap\n",
    "- Compare this confusion matrix with the one made in Task5\n",
    "\n",
    "*The input to this model will be the deep feature tensor obtained in Task6, so use appropriate input shape*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "TPCy0Mv3BZaH",
    "outputId": "1ee8c88e-2974-4f9b-a986-843264fea628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 60, 256)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-196c93616ab2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# reshaped=Reshape((num_classes,20))(input_im)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0moutput_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mydense_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmyModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
     ]
    }
   ],
   "source": [
    "# Those features are \n",
    "# Compare with task 5\n",
    "\n",
    "input_im = Input(shape=(8, 8, 20), name='input_im')\n",
    "\n",
    "reshaped=Reshape((64,20))(input_im)\n",
    "\n",
    "conv1 = Conv1D(20, kernel_size=(3), strides = (1),  activation='relu',name='my_conv1')(reshaped)\n",
    "conv2 = Conv1D(20, kernel_size=(3), strides = (1), activation='relu',name='my_conv2')(conv1)\n",
    "dense1 = Dense(256, activation='relu',name='mydense')(conv2)\n",
    "print(dense1.shape)\n",
    "# reshaped=Reshape((num_classes,20))(input_im)\n",
    "output_class = Dense(num_classes, activation='softmax',name='mydense_2')(dense1)\n",
    "print(output_class.shape)\n",
    "myModel = Model(inputs=input_im, outputs=output_class)\n",
    "myModel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2360
    },
    "colab_type": "code",
    "id": "h1DkbRETVqJ9",
    "outputId": "23e978ea-46dc-4e94-b3dc-7c4fe19c799c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "(79, 256, 256, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor Tensor(\"activation_21/Relu:0\", shape=(?, 8, 8, 20), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cb0fe696c2be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoded_gen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mland_use_generator2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallPaths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhist3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallPaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \"\"\"\n\u001b[0;32m--> 626\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e85f9c143391>\u001b[0m in \u001b[0;36mland_use_generator2\u001b[0;34m(allPaths, allLabels, y_true, num_classes, inpShape, encoder_model, resize, bath_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mbatch_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_meanShift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mencoded_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#             y_true += batch_labels_enc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1273\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1274\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    552\u001b[0m                                                \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                                                \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict_function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                                                **kwargs)\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Invalid argument \"%s\" passed to K.function with TensorFlow backend'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[1;32m   2544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2546\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m             \u001b[0mupdates_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[0;34m(control_inputs)\u001b[0m\n\u001b[1;32m   5002\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_NullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5003\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5004\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcontrol_dependencies\u001b[0;34m(self, control_inputs)\u001b[0m\n\u001b[1;32m   4541\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexedSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4542\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4543\u001b[0;31m       \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4544\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4545\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepL/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3567\u001b[0m       \u001b[0;31m# Actually obj is just the object it's referring to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3570\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3571\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOperation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"activation_21/Relu:0\", shape=(?, 8, 8, 20), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "encoder_model.compile(optimizer='adam',loss='mse')       # This will create predictions on all land\n",
    "myModel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "encoded_gen=land_use_generator2(allPaths,allLabels,y_true,num_classes,(256,256,3),encoder_model,True,batch_size)\n",
    "hist3 = myModel.fit_generator(encoded_gen, epochs=epochs, steps_per_epoch=len(allPaths)//batch_size,verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUhNzty4BZaJ"
   },
   "source": [
    "## Task 8\n",
    "\n",
    "Now you will explore another use of the deep features extracted in Task6. Content Based Image Retrieval (CBIR) is the task of searching for visually similar images from a dataset. *Think Google image search.* This concept can obviously be applied on other forms of data like text, audio or video as well. In this task you will:\n",
    "- Implement a function which will take three inputs and returns a list of visually similar images. The inputs would be\n",
    "> An image from the dataset `im` <br />\n",
    "The number of search results to return `n` (no more than 5) <br />\n",
    "A string representing the distance metric used for comparisons `dist`\n",
    "- Visualize search result images, by looking for the appropriate image\n",
    "- Use some images to compare the effects of these distance metrics on the output\n",
    "> Euclidean <br />\n",
    "Cosine <br />\n",
    "Mahalanobis\n",
    "\n",
    "*Make sure the query image is the first search result for your function*\n",
    "\n",
    "*Look up the documentation for Scipy's `spatial.distance` module. It is your best friend in this task.*\n",
    "\n",
    "*If you made a generator in Task6, you can very easily use it in this task as well*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-U_UlmcBZaK"
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def euc_dist(y_true, y_pred):\n",
    "    return K.sqrt(K.sum(K.square(y_true - y_pred), axis=-1, keepdims=True))\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cbir(im, n, dist):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AhdN1gkBZaO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw4d2_19100243.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}